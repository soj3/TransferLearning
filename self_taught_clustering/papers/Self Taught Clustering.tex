\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdfborder={0 0 0},
  breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-2}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\date{}

\begin{document}

For the domain of transfer learning, I implemented a self-taught
clustering, based on the paper ``Self-taught Clustering'' by W. Dai, Q.
Yang, Gui-Rong Xue, and Yong Yu. Self-taught clustering is an instance
of unsupervised learning. It uses a target data set \(X\), an auxiliary
data set \(Y\), and a common feature space between \(X\) and \(Y\),
\(Z\). The output is the optimal clustering of the target data set. The
sets \(\hat X, \hat Y, \hat Z\) are used to describe the sets of
clusters for each mapping (for example, \(\hat x_i\) is the
i\textsuperscript{th} cluster of data points belonging to \(X\). In
order to do this, it uses the auxiliary data set and common feature
space in order to improve its clustering algorithm at each iteration.
The authors use an extension of information theoretic co-clustering to
aid them in their self-taught clustering algorithm. The objective
function attempts to minimize the loss in mutual information between the
examples and features:

\[I(X, Z) - I(\hat X, \hat Z)\]

where \(I\) denotes the mutual information between the instances and
features. Both these can be simplified further:

\[I(X, Z) = \sum_{x\in X}\sum_{z\in Z}p(x, z)log(\frac{p(x, z)}{p(x)p(z)}) \\I(\hat X, \hat Z) = p(\hat x, \hat z) = \sum_{x\in\hat x}\sum_{z\in\hat z}p(x, z)\]

The self-taught clustering algorithm is using co-clustering to improve
the target clusters, meaning the auxiliary data \(Y\) is being used to
help improve the clustering of \(X\). The objective function is
therefore defined as

\[\tau = I(X, Z) - I(\hat X, \hat Z) + \lambda\left[I(Y, Z) - I(\hat Y, \hat Z)\right]\]

The mutual information involving \(Y\) and \(\hat Y\) is multiplied by a
trade-off parameter, \(\lambda\), in order to make sure the auxiliary
data does not have too much influence over the target data. Minimizing
the objective function is non-trivial, since it is non-convex. The
authors therefore used the Kullback-Leibler divergence (KL divergence)
in order to re-write the equation and try to minimized it.

In order to minimize this function, start by creating the joint
probability distribution of a variable and the feature space with
respect to their co-clusters:

\[\hat p(x, z) = p(\hat x, \hat z) p\left(\frac{x}{\hat x}\right)p\left(\frac{z}{\hat z}\right)\]

where \(p(\hat x, \hat z)\) is the joint probability distribution of
clusters \(\hat x, \hat z\), \(p\left(\frac{x}{\hat x}\right)\) is the
total number of features in \(x\) divided by the total number of
features in the cluster \(\hat x\), and
\(p\left(\frac{z}{\hat z}\right)\) is the total number of times the
features \(z\) appears divided by the total number of times each feature
in \(\hat z\) appears. The same equation can be written in terms of
\(Y\) to define the co-cluster joint probability distribution between
\(Y\) and \(Z\). The objective function can be re-written in the form of
KL divergence:

\[\tau = I(X, Z) - I(\hat X, \hat Z) + \lambda\left[I(Y, Z) - I(\hat Y, \hat Z)\right] = \\D(p(X, Z)||\hat p(X, Z)) + \lambda D(q(Y, Z)||\hat q(Y, Z))\]

where \(D(\frac{}{}||\frac{}{})\) denotes the KL divergence between the
probability distributions,

\[D(p(x), q(x)) = \sum_xp(x)log\left(\frac{p(x)}{q(x)}\right)\]

The KL divergence can be simplified for our objective function:

\[D(p(X, Z)||\hat p(X, Z)) = \sum_{\hat x\in \hat X}\sum_{x\in \hat x}p(x)D(p(Z|x)||\hat p(Z|\hat x))\]

which can be applied similarly for \(Y\):

\[D(p(Y, Z)||\hat p(Y, Z)) = \sum_{\hat y\in \hat Y}\sum_{y\in \hat y}p(y)D(p(Z|y)||\hat p(Z|\hat y))\]

So, by minimizing \(D(p(Z|x)||\hat p(Z|\hat x))\) for a single \(x\),
the optimization function will find a global minimum and the values of x
will be put into their respective clusters.

\end{document}
